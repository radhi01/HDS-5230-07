{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 12 - Neural Network"
      ],
      "metadata": {
        "id": "jFK2pPSQw2xb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The target variable (Y) in the dataset is continuous and takes on a wide range of numeric values, rather than representing discrete categories or labels. This indicates that the problem at hand is a regression task. Therefore, the appropriate approach is to use a deep learning regression model to predict the output based on the given input features."
      ],
      "metadata": {
        "id": "amdUan3Fw1Ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Explicit imports for the new model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l1\n",
        "\n",
        "# Other necessary imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error # To calculate MSE on original scale\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "i5xEQe7MxAKQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "avktgVYly9n_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "DATA_SIZES = [1000, 10000, 100000]\n",
        "CONFIGURATIONS_DL = [\n",
        "    {'name': '1 hidden layer 4 nodes', 'builder': 'simple', 'layers': [4]},\n",
        "    {'name': '2 hidden layers 4 nodes each', 'builder': 'simple', 'layers': [4, 4]},\n",
        "    # Add the new configuration\n",
        "    {'name': '3 hidden layers 64 nodes reg/dropout', 'builder': 'model4'}\n",
        "]\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "VALIDATION_SPLIT = 0.2\n",
        "RANDOM_STATE = 42\n",
        "L1_PENALTY = 0.1 # Define penalty for build_model4\n",
        "\n",
        "# List to store results\n",
        "dl_results = []"
      ],
      "metadata": {
        "id": "Cs4vmqoGTByz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for Simple DL Models\n",
        "def build_model_simple(n_features, layer_nodes, activation='relu'):\n",
        "    \"\"\"Builds the original simple Keras Sequential models.\"\"\"\n",
        "    model = Sequential(name=f\"{len(layer_nodes)}L_{'_'.join(map(str, layer_nodes))}N\")\n",
        "    model.add(keras.layers.Input(shape=(n_features,)))\n",
        "    for nodes in layer_nodes:\n",
        "        model.add(Dense(nodes, activation=activation))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mean_squared_error']) # Track MSE\n",
        "    return model\n",
        "\n",
        "# Provided Code in github, Model Builder Function\n",
        "def build_model4(input_shape_dim):\n",
        "    \"\"\"Builds the specified 3-layer network with L1/Dropout.\"\"\"\n",
        "    model = Sequential(name=\"3L_64N_RegDrop\") # Give it a unique name\n",
        "    model.add(Dense(64,\n",
        "                    activation='relu',\n",
        "                    kernel_regularizer=l1(L1_PENALTY), # Use defined penalty\n",
        "                    input_dim=input_shape_dim)) # Use passed dimension\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64,\n",
        "                    activation='relu',\n",
        "                    kernel_regularizer=l1(L1_PENALTY)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64,\n",
        "                    activation='relu',\n",
        "                    kernel_regularizer=l1(L1_PENALTY)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1,\n",
        "                    activation='linear')) # Linear output for regression\n",
        "    model.compile(optimizer='rmsprop', # As specified\n",
        "                  loss='mse',          # Use 'mse' for mean squared error loss\n",
        "                  metrics=['mae', 'mse']) # Track MAE and MSE during training\n",
        "    return model"
      ],
      "metadata": {
        "id": "5JCksouxTKy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o12SWvSJ0GG-",
        "outputId": "4dc82d13-763f-4778-fb35-4674008ea401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Processing Data Size: 1000 ---\n",
            "  Y variable stats for size 1000:\n",
            "count      1000.000000\n",
            "mean      45985.086248\n",
            "std       29024.553144\n",
            "min       -5145.718206\n",
            "25%       21964.626880\n",
            "50%       44883.537973\n",
            "75%       70894.279025\n",
            "max      105203.818034\n",
            "Name: Y, dtype: float64\n",
            "\n",
            "  --- Training Deep Learning Models ---\n",
            "\n",
            "    Training DL Model: 1 hidden layer 4 nodes\n",
            "      DL Training Finished.\n",
            "      DL Final Training Loss (scaled MSE): 0.0200\n",
            "      DL Final Validation Loss (scaled MSE): 0.0224\n",
            "      DL Final Training MSE (original scale): 16838507.5950\n",
            "      DL Final Validation MSE (original scale): 19117599.3749\n",
            "      DL Execution Time: 10.58 seconds\n",
            "\n",
            "    Training DL Model: 2 hidden layers 4 nodes each\n",
            "      DL Training Finished.\n",
            "      DL Final Training Loss (scaled MSE): 0.0434\n",
            "      DL Final Validation Loss (scaled MSE): 0.0438\n",
            "      DL Final Training MSE (original scale): 36404138.4583\n",
            "      DL Final Validation MSE (original scale): 37360560.1825\n",
            "      DL Execution Time: 11.92 seconds\n",
            "\n",
            "    Training DL Model: 3 hidden layers 64 nodes reg/dropout\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      DL Training Finished.\n",
            "      DL Final Training Loss (scaled MSE): 1.4236\n",
            "      DL Final Validation Loss (scaled MSE): 1.3623\n",
            "      DL Final Training MSE (original scale): 852027467.5871\n",
            "      DL Final Validation MSE (original scale): 799803699.1818\n",
            "      DL Execution Time: 12.68 seconds\n",
            "\n",
            "--- Processing Data Size: 10000 ---\n",
            "  Y variable stats for size 10000:\n",
            "count     10000.000000\n",
            "mean     453605.025645\n",
            "std      298868.066280\n",
            "min      -63153.513656\n",
            "25%      193556.409302\n",
            "50%      460534.064666\n",
            "75%      711530.726213\n",
            "max      998732.979548\n",
            "Name: Y, dtype: float64\n",
            "\n",
            "  --- Training Deep Learning Models ---\n",
            "\n",
            "    Training DL Model: 1 hidden layer 4 nodes\n",
            "      DL Training Finished.\n",
            "      DL Final Training Loss (scaled MSE): 0.0001\n",
            "      DL Final Validation Loss (scaled MSE): 0.0001\n",
            "      DL Final Training MSE (original scale): 5881283.4417\n",
            "      DL Final Validation MSE (original scale): 6195782.8417\n",
            "      DL Execution Time: 45.76 seconds\n",
            "\n",
            "    Training DL Model: 2 hidden layers 4 nodes each\n",
            "      DL Training Finished.\n",
            "      DL Final Training Loss (scaled MSE): 0.0000\n",
            "      DL Final Validation Loss (scaled MSE): 0.0000\n",
            "      DL Final Training MSE (original scale): 3608301.7112\n",
            "      DL Final Validation MSE (original scale): 4033195.5178\n",
            "      DL Execution Time: 47.54 seconds\n",
            "\n",
            "    Training DL Model: 3 hidden layers 64 nodes reg/dropout\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      DL Training Finished.\n",
            "      DL Final Training Loss (scaled MSE): 1.4224\n",
            "      DL Final Validation Loss (scaled MSE): 1.4171\n",
            "      DL Final Training MSE (original scale): 89418029883.5517\n",
            "      DL Final Validation MSE (original scale): 88894042681.9784\n",
            "      DL Execution Time: 55.51 seconds\n",
            "\n",
            "--- Processing Data Size: 100000 ---\n",
            "  Y variable stats for size 100000:\n",
            "count    1.000000e+05\n",
            "mean     4.495216e+06\n",
            "std      2.962349e+06\n",
            "min     -6.427087e+05\n",
            "25%      1.926356e+06\n",
            "50%      4.500260e+06\n",
            "75%      7.068165e+06\n",
            "max      9.735293e+06\n",
            "Name: Y, dtype: float64\n",
            "  WARNING: Target variable 'Y' has large values. Scaling is crucial.\n",
            "\n",
            "  --- Training Deep Learning Models ---\n",
            "\n",
            "    Training DL Model: 1 hidden layer 4 nodes\n",
            "      DL Training Finished.\n",
            "      DL Final Training Loss (scaled MSE): 0.0000\n",
            "      DL Final Validation Loss (scaled MSE): 0.0000\n",
            "      DL Final Training MSE (original scale): 44551305.7487\n",
            "      DL Final Validation MSE (original scale): 44301406.1709\n",
            "      DL Execution Time: 366.74 seconds\n",
            "\n",
            "    Training DL Model: 2 hidden layers 4 nodes each\n",
            "      DL Training Finished.\n",
            "      DL Final Training Loss (scaled MSE): 0.0000\n",
            "      DL Final Validation Loss (scaled MSE): 0.0000\n",
            "      DL Final Training MSE (original scale): 38298279.4608\n",
            "      DL Final Validation MSE (original scale): 38157371.6382\n",
            "      DL Execution Time: 363.59 seconds\n",
            "\n",
            "    Training DL Model: 3 hidden layers 64 nodes reg/dropout\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      DL Training Finished.\n",
            "      DL Final Training Loss (scaled MSE): 1.4222\n",
            "      DL Final Validation Loss (scaled MSE): 1.4246\n",
            "      DL Final Training MSE (original scale): 8772232717850.7393\n",
            "      DL Final Validation MSE (original scale): 8791813609110.1631\n",
            "      DL Execution Time: 452.31 seconds\n",
            "\n",
            "======================================================================\n",
            "               DEEP LEARNING RESULTS SUMMARY\n",
            "======================================================================\n",
            " Data size                        Configuration  Training error (MSE)  Validation error (MSE)  Time of execution (s)\n",
            "      1000               1 hidden layer 4 nodes           16838507.60             19117599.37                  10.58\n",
            "      1000         2 hidden layers 4 nodes each           36404138.46             37360560.18                  11.92\n",
            "      1000 3 hidden layers 64 nodes reg/dropout          852027467.59            799803699.18                  12.68\n",
            "     10000               1 hidden layer 4 nodes            5881283.44              6195782.84                  45.76\n",
            "     10000         2 hidden layers 4 nodes each            3608301.71              4033195.52                  47.54\n",
            "     10000 3 hidden layers 64 nodes reg/dropout        89418029883.55          88894042681.98                  55.51\n",
            "    100000               1 hidden layer 4 nodes           44551305.75             44301406.17                 366.74\n",
            "    100000         2 hidden layers 4 nodes each           38298279.46             38157371.64                 363.59\n",
            "    100000 3 hidden layers 64 nodes reg/dropout      8772232717850.74        8791813609110.16                 452.31\n",
            "======================================================================\n",
            "\n",
            "--- Analysis ---\n",
            "\n",
            "1. Best Deep Learning Model:\n",
            "Based on the lowest validation MSE (original scale), the best performing deep learning model is:\n",
            "Data size                                        10000\n",
            "Configuration             2 hidden layers 4 nodes each\n",
            "Training error (MSE)                    3608301.711227\n",
            "Validation error (MSE)                  4033195.517802\n",
            "Time of execution (s)                        47.537414\n",
            "\n",
            "Reasoning: This configuration ('2 hidden layers 4 nodes each' on data size 10000) achieved the lowest validation MSE (4033195.5178 on the original scale). This estimates the best performance on unseen data.\n"
          ]
        }
      ],
      "source": [
        "# Main Loop for Data Sizes\n",
        "for size in DATA_SIZES:\n",
        "    print(f\"\\n--- Processing Data Size: {size} ---\")\n",
        "    data_file = f'data_{size // 1000}k.csv'\n",
        "\n",
        "    if not os.path.exists(data_file):\n",
        "        print(f\"ERROR: Data file {data_file} not found. Please generate it first.\")\n",
        "        # Add placeholder results for all configurations for this size\n",
        "        for config in CONFIGURATIONS_DL:\n",
        "             dl_results.append({\n",
        "                'Data size': size,\n",
        "                'Configuration': config['name'],\n",
        "                'Training error (MSE)': np.nan,\n",
        "                'Validation error (MSE)': np.nan,\n",
        "                'Time of execution (s)': np.nan\n",
        "            })\n",
        "        continue\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv(data_file)\n",
        "    X = df[['X1', 'X2', 'X3', 'X4']]\n",
        "    y = df['Y']\n",
        "\n",
        "    # Check for extreme values in Y (optional but good practice)\n",
        "    print(f\"  Y variable stats for size {size}:\")\n",
        "    print(y.describe())\n",
        "    if y.abs().max() > 1e6: # Warning if Y values are very large\n",
        "         print(\"  WARNING: Target variable 'Y' has large values. Scaling is crucial.\")\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=VALIDATION_SPLIT, random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    # Scale Features (X)\n",
        "    x_scaler = StandardScaler()\n",
        "    X_train_scaled = x_scaler.fit_transform(X_train)\n",
        "    X_val_scaled = x_scaler.transform(X_val)\n",
        "    n_features = X_train_scaled.shape[1]\n",
        "\n",
        "    # Scale Target (Y) - IMPORTANT for large Y values\n",
        "    y_scaler = StandardScaler()\n",
        "    y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "    y_val_scaled = y_scaler.transform(y_val.values.reshape(-1, 1))\n",
        "\n",
        "    # Deep Learning Training Loop\n",
        "    print(\"\\n Training Deep Learning Models \")\n",
        "    for config in CONFIGURATIONS_DL:\n",
        "        print(f\"\\n    Training DL Model: {config['name']}\")\n",
        "\n",
        "        # Build the appropriate model based on configuration\n",
        "        if config['builder'] == 'model4':\n",
        "            model = build_model4(n_features)\n",
        "        elif config['builder'] == 'simple':\n",
        "            model = build_model_simple(n_features, config['layers'])\n",
        "        else:\n",
        "            print(f\"      ERROR: Unknown builder type '{config.get('builder', 'N/A')}'\")\n",
        "            continue # Skip this configuration\n",
        "\n",
        "        # print(model.summary()) # Optional: uncomment to see model structure\n",
        "\n",
        "        start_time = time.time()\n",
        "        # Train on SCALED features and SCALED target\n",
        "        history = model.fit(\n",
        "            X_train_scaled, y_train_scaled,\n",
        "            validation_data=(X_val_scaled, y_val_scaled),\n",
        "            epochs=EPOCHS,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            verbose=0 # Suppress epoch output\n",
        "        )\n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - start_time\n",
        "\n",
        "        # Get loss from history (which is MSE as defined in compile)\n",
        "        final_train_loss_scaled = history.history['loss'][-1]\n",
        "        final_val_loss_scaled = history.history['val_loss'][-1]\n",
        "        print(f\"      DL Training Finished.\")\n",
        "        print(f\"      DL Final Training Loss (scaled MSE): {final_train_loss_scaled:.4f}\")\n",
        "        print(f\"      DL Final Validation Loss (scaled MSE): {final_val_loss_scaled:.4f}\")\n",
        "\n",
        "        # Calculate MSE on ORIGINAL scale\n",
        "        # Predict using scaled inputs\n",
        "        pred_train_scaled = model.predict(X_train_scaled, verbose=0)\n",
        "        pred_val_scaled = model.predict(X_val_scaled, verbose=0)\n",
        "\n",
        "        # Inverse transform predictions to get them back to original Y scale\n",
        "        pred_train_orig = y_scaler.inverse_transform(pred_train_scaled)\n",
        "        pred_val_orig = y_scaler.inverse_transform(pred_val_scaled)\n",
        "\n",
        "        # Calculate MSE between original y and inverse-transformed predictions\n",
        "        final_train_error_orig = mean_squared_error(y_train, pred_train_orig)\n",
        "        final_val_error_orig = mean_squared_error(y_val, pred_val_orig)\n",
        "\n",
        "        print(f\"      DL Final Training MSE (original scale): {final_train_error_orig:.4f}\")\n",
        "        print(f\"      DL Final Validation MSE (original scale): {final_val_error_orig:.4f}\")\n",
        "        print(f\"      DL Execution Time: {execution_time:.2f} seconds\")\n",
        "\n",
        "        # Store results (using original scale MSE for reporting)\n",
        "        dl_results.append({\n",
        "            'Data size': size,\n",
        "            'Configuration': config['name'],\n",
        "            'Training error (MSE)': final_train_error_orig,\n",
        "            'Validation error (MSE)': final_val_error_orig,\n",
        "            'Time of execution (s)': execution_time\n",
        "        })\n",
        "\n",
        "# Display Results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"               DEEP LEARNING RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "dl_results_df = pd.DataFrame(dl_results)\n",
        "# Format MSE columns for better readability if they are very large or small\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "print(dl_results_df.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Analysis Section\n",
        "print(\"\\n Analysis \")\n",
        "if not dl_results_df.dropna().empty:\n",
        "    best_dl_model_idx = dl_results_df['Validation error (MSE)'].idxmin()\n",
        "    best_dl_model_info = dl_results_df.loc[best_dl_model_idx]\n",
        "    print(\"\\n1. Best Deep Learning Model:\")\n",
        "    print(f\"Based on the lowest validation MSE (original scale), the best performing deep learning model is:\")\n",
        "    # Use default float format for printing the best model info row\n",
        "    pd.reset_option('display.float_format')\n",
        "    print(best_dl_model_info.to_string())\n",
        "    pd.set_option('display.float_format', '{:.2f}'.format) # Reset for any later prints\n",
        "    print(f\"\\nReasoning: This configuration ('{best_dl_model_info['Configuration']}' on data size {best_dl_model_info['Data size']}) achieved the lowest validation MSE ({best_dl_model_info['Validation error (MSE)']:.4f} on the original scale). This estimates the best performance on unseen data.\")\n",
        "else:\n",
        "    print(\"\\n1. Best Deep Learning Model: No valid DL results were generated.\")\n",
        "\n",
        "# Reset float format if needed elsewhere\n",
        "pd.reset_option('display.float_format')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the deep learning experiments, the 2-layer, 4-node configuration consistently delivered the best performance across different data sizes. This architecture struck an effective balance between low validation error and reasonable execution time, making it the most reliable among the tested deep learning setups. On the other hand, more complex deep models specifically those with 3 hidden layers, 64 nodes per layer, and regularization/dropout performed very poorly. These models produced extremely high training and validation MSE, likely due to overfitting, improper regularization, or general instability during training."
      ],
      "metadata": {
        "id": "CZVBmHusUo0U"
      }
    }
  ]
}